\chapter{Tree sequences as a general-purpose tool for population genetic inference}

\\
\noindent{\textbf{Authors:} Whitehouse LS, Ray D, Schrider DR}\\
\textbf{Preprint Manuscript:} Whitehouse LS, Ray D, Schrider DR. Tree sequences as a general-purpose tool for population genetic inference. doi: https://doi.org/10.1101/2024.02.20.581288

\section{Introduction}

Recent decades have seen dramatic growth in the volume, scale, and diversity of population genetic datasets. This explosion in the amount of data has coincided with the development of an expansive set of statistical methods designed to make more accurate and detailed evolutionary inferences from this glut of data. Initial methods often relied on examining the values of statistics that summarize relevant aspects of genetic diversity \cite{fuStatisticalTestsNeutrality1993,neiMathematicalModelStudying1979,tajimaStatisticalMethodTesting1989,wattersonNumberSegregatingSites1975}. For example, statistics quantifying information about the presence of high-frequency derived alleles may be used to identify recent positive selection, which produces an excess of high-frequency derived alleles via genetic hitchhiking \cite{fayHitchhikingPositiveDarwinian2000}. In addition to statistics that summarize the distribution of allele frequencies in a population, researchers have designed statistics to capture other properties of genetic variation as well, such as the density of polymorphism \cite{neiMathematicalModelStudying1979,neiDNAPolymorphismDetectable1981}, patterns of haplotypic diversity \cite{garudRecentSelectiveSweeps2015,hudsonEvidencePositiveSelection1994,sabetiDetectingRecentPositive2002,voightMapRecentPositive2006}, and the extent of linkage disequilibrium \cite{kellyTestNeutralityBased1997,kimLinkageDisequilibriumSignature2004}. Because such summary statistics necessarily result in the loss of potentially valuable information by representing population genomic diversity by a single number, more recent approaches, such as approximate Bayesian computation (ABC) sought to combine multiple summaries into a higher-dimensional representation of the data that could then be used for inference \cite{beaumontApproximateBayesianComputation2002,pritchardPopulationGrowthHuman1999,tavareInferringCoalescenceTimes1997}. ABC methods work by comparing large sets of simulations, each summarized by a vector of statistics, to identify the evolutionary model and/or parameter combination that best matches the observed data. Importantly, the summary statistic vector can capture a variety of properties of genetic variation, that together may be informative about the evolutionary models/parameters being considered.

More recently, researchers have begun developing supervised machine learning (SML) methods for population genetic inference \cite{korfmannDeepLearningPopulation2023a,schriderSupervisedMachineLearning2018}. Unlike traditional applications of supervised learning, these approaches use simulated data to train and test methods before applying them to empirical datasets. This reliance on simulated training data makes SML approaches to population genetic inference similar in principle to ABC, although these methods often have greater accuracy and computational efficiency \cite{pudloReliableABCModel2016,raynalABCRandomForests2019}. SML methods have been applied to a variety of population genetic tasks including detecting positive selection \cite{arnabUncoveringFootprintsNatural2022,hejaseDeepLearningApproachInference2022,kernDiploSHICUpdated2018,linDistinguishingPositiveSelection2011,mughalLocalizingClassifyingAdaptive2019,mughalLearningPropertiesAdaptive2020,pavlidisSearchingFootprintsPositive2010,pybusHierarchicalBoostingMachinelearning2015,ronenLearningNaturalSelection2013,whitehouseTimesweeperAccuratelyIdentifying2022}, demographic parameter estimation \cite{sheehanDeepLearningPopulation2016,wangAutomaticInferenceDemographic2021} and model inference \cite{sanchezDeepLearningPopulation2021}, inferring recombination rates \cite{adrionPredictingLandscapeRecombination2020,gaoNewSoftwareFast2016}, detecting introgression \cite{gowerDetectingAdaptiveIntrogression2021,rayIntroUNETIdentifyingIntrogressed2023,schriderSupervisedMachineLearning2018a}, and numerous other applications \cite{batteyPredictingGeographicLocation2020,batteyVisualizingPopulationStructure2021,bookerThisPopulationDoes2023,smithDispersalInferencePopulation2023,yelmenCreatingArtificialHuman2021}. The initial applications of SML to population genetic problems took as their input a vector of summary statistics, and yielded sizable accuracy gains on several tasks (reviewed in \cite{schriderSupervisedMachineLearning2018}). Because representing genetic diversity within a population by a set of statistics may still result in valuable information being lost, researchers have more recently begun to experiment with deep learning methods that are capable of directly examining a population genetic alignment as input and learning their own set of features to extract from these data \cite{chanLikelihoodFreeInferenceFramework2018,flagelUnreasonableEffectivenessConvolutional2019}. Although deep learning has allowed for a multitude of significant advances in population genetics (reviewed in \cite{korfmannDeepLearningPopulation2023a}), one important drawback of this approach is that population genetic alignments are an inefficient format for representing data \cite{kelleherInferringWholegenomeHistories2019}. Because of the large amount of information that must be stored in onboard GPU memory, examining large genomic regions or entire chromosomes and/or large sample sizes is a significant computational challenge for deep learning methods that take alignments as their input.

One approach to alleviate this issue is to store genetic information as tree sequences: the sequence along the chromosome of genealogies describing the shared evolutionary history of the sequenced genomes \cite{kelleherEfficientCoalescentSimulation2016}, with adjacent genealogies in this sequence separated by the breakpoints of recombination events that resulted in a change to the sample's genealogy \cite{wiufRecombinationPointProcess1999}. Tree sequences can be used to concisely store all of the information present in a population genetic alignment, and this innovation has led to dramatic improvements in simulation efficiency \cite{hallerSLiMMultispeciesEcoEvolutionary2022,kelleherEfficientPedigreeRecording2018}. Relatedly, tree sequences can be used as a hyper-efficient form of lossless compression for population genetic data \cite{kelleherInferringWholegenomeHistories2019}, and can in principle describe the near-complete evolutionary history of a sample. These advantages have motivated the development of methods to use sequence data to infer tree sequences \cite{kelleherInferringWholegenomeHistories2019,speidelMethodGenomewideGenealogy2019a,zhangBiobankscaleInferenceAncestral2023a} or ancestral recombination graphs (ARGs; \cite{mahmoudiBayesianInferenceAncestral2022,rasmussenGenomeWideInferenceAncestral2014}), a similar but more complete representation of a sample's history. 

In addition to inferring tree sequences from data, there is growing interest in conducting inference on the tree sequences themselves. For example, tree sequences have been used to efficiently compute commonly used summary statistics with dramatic improvements in speed and memory usage \cite{ralphEfficientlySummarizingRelationships2020}. Researchers have also begun developing methods using tree sequences or summaries thereof as input to machine learning methods for evolutionary inference \cite{hejaseDeepLearningApproachInference2022,korfmannSimultaneousInferenceDemography2023,pearsonLocalAncestryInference2023}. Given that when applied to real datasets such methods will be run on tree sequences that are inferred with a considerable amount of error \cite{zhangBiobankscaleInferenceAncestral2023a} one concern may be that tree-sequence methods using erroneous input information may produce less satisfactory results than methods directly acting on the input sequence data. However despite this potential drawback SML methods that are trained to handle potential errors in their tree-sequence input could potentially circumvent this problem \cite{moDomainadaptiveNeuralNetworks2023a}, resulting in successful use of SML on inferred tree sequences.

With these potential strengths and weaknesses of tree sequence-based inference in mind, we sought to assess whether deep learning approaches trained on tree sequences are likely to add value in comparison to methods acting directly on population genetic alignments. We address this question by assessing the performance of graph convolutional networks (GCNs), a generalization of convolutional neural networks (CNNs) to handle graphical input data. We train GCNs to solve common tasks in population genetic inference: detecting loci that have experienced introgression between closely related species, identifying selective sweeps, estimating recombination rates, and inferring demographic parameters (the same tasks examined previously by \cite{flagelUnreasonableEffectivenessConvolutional2019}). Encouragingly, we find that GCNs trained to use inferred tree sequences yield equivalent or better accuracy to those trained on population genetic alignments. We close with a discussion of the implications of our GCN's performance for future population genetic research leveraging this increasingly commonly used representation of evolutionary histories and genetic variation data.


\section{Methods}

\subsection{Tree sequence inference}

For each problem, simulations were performed as described in the sections below, producing ms-style output. The simulation output was then formatted for input to RELATE \cite{speidelMethodGenomewideGenealogy2019a}, a recently developed method that, when given a sample of genomes along with estimate of the mutation rate, recombination rate, and effective population size, infers the sample's genome-wide genealogy in the form of a sequence of rooted binary trees. RELATE outputs this sequence of trees in order along the chromosome, with each tree spanning a variable length of sequence bounded by the breakpoints of inferred historical recombination events that altered the tree topology. This was done for each simulation replicate for each problem described below, using the mutation, recombination rate and effective population size parameters listed in Table B.1. 

\subsection{Overview of Graph Convolutional Networks (GCNs)}
Graph convolution \cite{kipfSemiSupervisedClassificationGraph2017} works by aggregating information at each node in a graph by combining data from its neighbors (those nodes with which it shares an edge). The input to a graph convolution operator is a graph or a set of nodes and edges (N, E), where each is some n-dimensional vector containing information about a given node, and each element of E is a pair of node indices representing a connection between those two nodes. The edges of a graph are often specified instead by an adjacency matrix which specifies the topology of the graph: i.e. has a value of 1 at entry if node and node share an edge and a value of 0 otherwise. Formally the graph convolution can be written in matrix form as:
$$\mathbf{X}^{\prime} = \mathbf{\hat{D}}^{-1/2}\mathbf{\hat{A}}\mathbf{\hat{D}}^{-1/2} \mathbf{X} \mathbf{\Theta} \;(1)$$
where $X$ is a matrix consisting of the $n$ node feature vectors $\mathbf{\hat{A}} = \mathbf{A} + \mathbf{I}$ with $I$ as the identity matrix and $\hat{D}_{ii} = \sum_{j=0} \hat{A}_{ij}$ as the diagonal degree matrix. Here features are first aggregated according to the connectivity specified in $A$ including the features belonging to the node itself (hence the addition of the identity matrix specifying self-connectivity, a “self-loop”) and then normalized using the degree matrix to avoid exploding gradients. Then, they are transformed according to some learnable matrix $\Theta$ optimized through gradient descent during training. For clarity, the node-wise formula is: 
$$\mathbf{x}^{\prime}_i = \mathbf{\Theta}^{\top} \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\hat{d}_j \hat{d}_i}} \mathbf{x}_j \;(2)$$

where the degree of node (including self-connectivity) $i$ is $\hat{d}_i = 1 + |\mathcal{N}(i)|$, with $\mathcal{N}(i)$ being the set of neighbors of node $i$, and $x_i$ being the node features for node $i$. Thus, in each graph convolution step, each node's features are updated to as weighted sum of all its neighbors features including its own (i.e. the features of the set of nodes $*$, as specified in the summation). Specifically, where each contribution of each node's feature vector $j$ is weighted by the reciprocal of the square root of the product of node $*'$ sits degree and node $i$'s degree and the degree of its neighbor, as in the denominator of Equation 2. Then, that resulting sum is transformed according to the learnable matrix $\Theta$. This is called a “convolution” on graphs as it is analogous to convolutions on images where the “kernel” of a graph convolution operates over each local neighborhood of each node much like an image convolution kernel operates on some rectangular region around each pixel (see example in Figure B.1). 

In the case of genealogical trees, the node features returned by each graph convolution operation are a function of those of its parent and its two children if it is an internal node, its two children only if it is the root node, and its parent only if it is a leaf node. For our input graphs we used the following features: the age of a node (the time in generations that the coalescence event associated with it occurred, or 0 for leaf nodes), the number of mutations on the branch from its parent (mean and standard deviation normalized via the training data), and a one-hot encoded label vector specifying whether the node is internal (i.e. ancestral) or external (present day) respectively, and the population label if the node is external (for internal nodes the population is considered unknown). Thus, if there are k populations the size of the label vector is $k + 1$. Note that the definition of graph convolution allows for directionality of the edges, i.e. one can specify whether node i passes information to node j in each graph convolution operation or vice-versa. Here we specify that the edges are unidirectional with the direction being child to parent. Thus, for our GCN $\mathcal{N}(i)$ is the set of child nodes of node $i$, and no update is performed for external nodes in the tree $\mathcal{N}(i)$ is the empty set, and thus for those nodes $\mathcal{N}(i)\cup \left\{i \right\} = \left\{i \right\}$, meaning only the focal node is considered during the convolution. 

\subsection{GCN Network Architecture and training}
 
The GCN architecture implemented here (illustrated in Fig. 1) consists of two components: a graph-learning network, or layers that operate on graphs, and two recurrent networks that operate on sequences of feature vectors. First, each tree in the input sequence has its node features transformed via a learned affine transform with bias, to which the original features are concatenated to via a “skip connection” before each tree is then input separately to a series of independent graph convolution layers with skip connections, comprising a graph convolution “block” of layers. At each layer, the output from the previous layer is transformed via convolution and added to itself, then the result is normalized and passed to the ReLU activation function \cite{agarapDeepLearningUsing2018}. For normalization in the GCN layers we use LayerNorm \cite{baLayerNormalization2016}. The graph convolution layer we chose for our architecture is the Graph Attention Convolution (GATConv) from \cite{brodyHowAttentiveAre2022} implemented in Pytorch \cite{paszkePyTorchImperativeStyle2019}. It is a modified form of the proposed convolution operator that we detail in the previous section in that node features from each neighbor are linearly transformed before they are summed in the update step and are weighted by learned attention coefficients instead of simply the degree of the nodes, enabling the network to potentially learn which neighbor's features are more or less relevant for accomplishing the given regression task. In short, weighting by node degrees as described in the previous section prevents gradient explosion whereas weighting by attention prevents gradient explosion and has the benefit of learning the most optimally-informative connections in the graph for a given task \cite{brodyHowAttentiveAre2022}. Formally the convolution with attention is specified as: 

$$\mathbf{x}^{\prime}_i = \alpha_{i,i}\mathbf{\Theta}_{s}\mathbf{x}_{i} + \sum_{j \in \mathcal{N}(i)} \alpha_{i,j}\mathbf{\Theta}_{t}\mathbf{x}_{j} \;(3)$$

Where the attention coefficients $\alpha$ are computed as:

$$\alpha_{i,j} = \frac{\exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_j\right)\right)}{\sum_{k \in \mathcal{N}(i) \cup \{ i \}}\exp\left(\mathbf{a}^{\top}\mathrm{LeakyReLU}\left(\mathbf{\Theta}_{s} \mathbf{x}_i + \mathbf{\Theta}_{t} \mathbf{x}_k \right)\right)} \;(4)$$

With $\mathbf{a} \in \mathbb{R}^{d}$ and $\mathbf{\Theta}{s}, \mathbf{\Theta}_{t} \in \mathbb{R}^{d^{\prime} \times d}$ being learnable parameters weighted during training where $d$ and $d^{\prime}$ are the dimension of the input features and output features respectively and Rd being a d-dimensional vector of real numbers. In each GATConv layer, attention coefficients $\alpha_{i,j}$ are computed which selectively filter messages from neighboring nodes. This attention mechanism is comprised of the same two learnable affine transformation matrices used in eq. 5, $\mathbf{\Theta}_{s}$ and $\mathbf{\Theta}_{t}$, which transform the focal node and its neighbor’s node features respectively and a being a trainable weight vector onto which the summed features are projected via the dot product and then normalized over all neighbors to sum to one using the softmax function. In this way, the attention mechanism hopes to weight the messages from neighboring nodes with $\alpha_{i,j}$ such that returned features have the most impact on loss reduction for a given target. Note that self-attention coefficients expressing how much the focal node’s own features contribute to the features in the next layer ($\alpha_{i,i}$) are also computed in a similar manner.

After passing through the GATConv layers, features obtained from the nodes in each tree are separately passed through a Gated Recurrent Unit (GRU; \cite{choLearningPhraseRepresentations2014}) and the hidden features are returned as the compressed vector representation of each tree in the sequence. For each GRU's input, the order of nodes in the tree is the same order that is returned by RELATE in Newick format. The vector emitted by each GRU is concatenated to a “tree summary vector” containing the following features about the original tree: the time of coalescence of all individuals in the tree, the mean, median, and standard deviation of coalescence times, the mean, median, standard deviation, skew, and maximum of the branch lengths in the tree, the midpoint of the chromosomal segment the tree corresponds to (scaled from 0 to 1, with 0 being the left end of the simulated chromosome and 1 being the right end), and the fraction of the entire sequence accounted for by this segment. The resulting vectors, one for each input tree, are then concatenated into a single matrix, which is in turn passed through another GRU that emits a feature vector summarizing the entire tree sequence. A global vector (the “tree sequence summary vector”) which consists of the mean, standard deviation, and median of the tree summary vectors, as well as the number of graphs in the original sequence, is embedded using an affine transform with bias and then concatenated to the resulting hidden vector from the previous GRU, resulting in a 1D vector containing learned information from the graph convolution process and summary statistics about the tree sequence as a whole. Each global vector and pre-computed tree summary vector for the graphs is mean and standard deviation normalized via the training data set in preprocessing. Finally, this is passed through three fully connected layers (a multi-layer perceptron, “MLP”) with the first two using ReLU activations and batch normalization \cite{ioffeBatchNormalizationAccelerating2015}, and the final (output) layer using no activation function. This results in classification scores which could be interpreted as posterior probabilities by applying the softmax function, or in the case of regression, z-scores for the various values to be predicted (all values for the regression tasks are standardized via the mean and standard deviation computed from the training set). We provide a table specifying the dimensionality of the inputs and outputs in each layer of our network as well as the activations used for clarity (Table 2.1).

When passing the ragged sequences to the final GRU layer they are each padded to the maximum length observed in the given batch. For all the experiments however, we first downsample the tree sequences if they are > 128 trees in length by randomly selecting a sequential window of 128 trees and omitting the others. This cap on tree sequence lengths allowed us to increase the batch size while still being able to train using a single GPU. For some tasks, namely in the case of recombination rate and demographic parameter regression, this is far more than the average number of trees observed in the chosen window size and thus most tree sequences are passed in full to the network. However, in the case of the sweep detection problem, there are often far more than 128 trees in the tree sequence, and thus many of the replicates passed to the network are downsampled sequences. Further details of how we downsample the tree sequences and the average amount of downsampling are given in the sections detailing each task. 
 
For classification tasks, we used categorical cross entropy as our loss function to be minimized during training: $\frac{1}{N}\sum_{i=1}^{N}\sum_{c=1}^{M}{y_{i,c}}\log{y_{i,c}} \;(5)$

Where $y_{i,c}$ is a binary indicator of the correct class and $y_{i,c}$ is our posterior probability estimate that the observation came from this class where $i$ is the replicate index, $c$ is the class index, $M$ is the number of classes, and $$N$$ is the batch size or number of replicates considered in each gradient step. For regression tasks, we use a variant of L1 loss (i.e. mean absolute error (MAE)) because of L1's relative insensitivity to outliers; specifically, we used the “smooth L1” loss function, whose slope decays at values closer to zero \cite{girshickFastRCNN2015}. However, note that for all methods and tasks we report final regression accuracies in root mean-squared error (RMSE) to better compare to Flagel et al. \cite{flagelUnreasonableEffectivenessConvolutional2019}. $y$ values for the regression tasks are standardized via the mean and standard deviation computed from the training set. We trained using the ADAM optimizer \cite{kingmaAdamMethodStochastic2017} with a learning rate of $1e-4$ for up to 100 epochs or until their validation accuracy stopped increasing for 10 consecutive epochs. For each problem we specify the batch size in the sections below.


Because input tree sequences are of variable length, here we define the dimensionality of a single batch as $(B, T\times N)$ where $B$ is the batch size (number of tree sequences in a batch) and $T \times N$ is the number of trees $\times$ the number of nodes. For each node, we calculate a set of $F$ features, giving us an input shape of $(B, T \times N, F)$ as shown in the “input shape” column for the first layer Table 2.1 (the “node embedding” row). The inputs to the graph convolution layers are a 2-D tensor of shape $(T \times N, 26+F)$, where $T\times N$ is again the total number of nodes in the tree sequence, and 26 features per node resulting from concatenated with the original node features from the first skip connection), and a list of edge indices of shape $(2, E)$ specifying the pairwise connections between nodes where each row is a pair of node indices the edge connects, similar to an adjacency matrix this information is used by the graph convolution layers to utilize the node connectedness. The input and output shapes of the skip connection layer, reshaping layer, and first GRU layer that follow the graph convolution layers are also determined by $T$, $N$, and $F$ as shown in Table 2.1. When considering the input to the second GRU layer, $T'$ refers to the padded tree sequence length, which is the maximum observed number of trees among the $B$ tree sequences in a given batch. This padding is performed because all tree sequences in a given batch must have the same length, and we chose to increase the length of tree sequences to $T'$ by concatenating tensors filled with zeros to the end of the GRU's input tensor rather than downsampling the tree sequence by removing trees. Note that this padding was done using PyTorch's pack\_padded\_sequence function that allows for more efficient computation by allowing $T'$ to vary across batches rather than setting it to the maximum length across all batches and instructing the GRUs to ignore the padded part of the sequence during forward and backward computations \cite{paszkePyTorchImperativeStyle2019}. This is followed by another concatenation layer and then several fully connected layers culminating in the output layer. The input and output shapes for all of these are shown in Table 2.1. All benchmark tasks except the introgression-detection task use identical output shapes for all layers except the final output layer, which is task-dependent. In total, the chosen architecture has 228,837 parameters for the demography and positive selection problems, 229,771 parameters for the introgression classification problem, and 228,381 parameters for the recombination rate problem.

\subsection{CNN training}
The CNN used for comparison was the Pytorch \cite{paszkePyTorchImperativeStyle2019} implementation of the ResNet34 architecture \cite{heDeepResidualLearning2015}. ResNets are a type of CNN that feature skip or residual connections that merge output from the convolutions with the output from previous layers via addition, and have been shown to perform competitively on traditional image recognition tasks \cite{reddiMLPerfInferenceBenchmark2020}. The model architecture is illustrated in Figure B.2. Models were trained for up to 100 epochs or until their validation accuracy stopped increasing for 10 consecutive epochs. For classification tasks (selection detection, introgression detection) categorical cross entropy loss was used, while smooth L1 \cite{girshickFastRCNN2015} loss was used for regression tasks (rho estimation, demographic inference). The ADAM optimizer with default settings (lr=0.001, betas=(0.9, 0.999)) \cite{kingmaAdamMethodStochastic2017} was used for all networks. All genotype matrix inputs were formatted to be of shape (populations, individuals, polymorphisms) and were padded with zeros to the max observed number of polymorphisms in each case. The rows were sorted based on cosine distance using seriation as described in \cite{rayIntroUNETIdentifyingIntrogressed2023} using Googles OR-tools package \cite{perronORTools2019}; note that the sorting algorithm and similarity metric differ from that used by \cite{flagelUnreasonableEffectivenessConvolutional2019} as we found the seriation approach to perform better on the task of identifying introgressed haplotypes \cite{rayIntroUNETIdentifyingIntrogressed2023}. Data was represented as binary encoding of 0 or 1 (ancestral or derived) for all tasks. 
The batch size for the CNN for both regression cases was 32. The batch size for the selection problem was 10 as the padded size was very large at 5000 SNPs. The batch size for the introgression problem was 96. The CNN used for comparison was the ResNet34 architecture \cite{heDeepResidualLearning2015} which was implemented in PyTorch. ResNets are a type of CNN that feature skip or residual connections that merge output from the convolutions with the output from previous layers via addition, and have been shown to perform competitively on traditional image recognition tasks. 

\subsection{Test Cases}
\subsubsection{Inferring the historical recombination rate}

Simulations for training and benchmarking performance on recombination rate inference were done using ms \cite{hudsonGeneratingSamplesWright2002} with the same simulation commands used by Flagel et al. \cite{flagelUnreasonableEffectivenessConvolutional2019}; these commands are available at https://raw.githubusercontent.com/flag0010/pop\_gen\_cnn/master/historical\_recombination. In particular, for each simulation replicate a population size N was drawn from the set {5000, 10,000, 20,000, 50,000}, and, after setting the locus size to L=50 kb and the mutation rate to $\mu=1.5\times10^{-8}$, a recombination rate $\rho=4NrL$ was chosen such that the per-base pair crossover rate, $r$, was drawn from a truncated exponential distribution with lower and upper boundaries of $10^{-8}$ and $10^{-6}$, respectively. The sample size was set to 50 haploid chromosomes. Simulation and formatting resulted in 189,528 replicates for training the CNN, and 9,956 for its validation.

Recombination rates values were log-scaled and then standardized to a z-score using the training set mean and standard deviation. Note that the validation and test data were also standardized using the training set's mean and standard deviation to prevent data leakage from the training to the validation/test data. Because the number of segregating sites differed across simulation replicates, the data was padded to 376 polymorphisms or the maximum observed number of polymorphisms in the simulated replicates. The median number of trees returned by RELATE (computed from training data) was 13 with a standard deviation of 24.05. For the GCN, the training and validation sets had 174,575 and 18,625 replicates respectively. The test set used to compare the two approaches had 9,715 replicates.

\subsubsection{Detecting selective sweeps}

We benchmarked accuracy on the task of detecting selective sweeps by adopting the approach of \cite{flagelUnreasonableEffectivenessConvolutional2019,schriderHICRobustIdentification2016} in that we define the problem as a classification task with 5 classes: hard sweeps (a sweep of a de novo mutation), hard-linked regions (i.e. those close to a hard sweep), soft sweeps (selection on standing variation), soft-linked regions (close to a soft sweep), and neutrally evolving regions. Sweeps were simulated using discoal \cite{kernDiscoalFlexibleCoalescent2016} with the same simulation parameters used by Schrider and Kern \cite{schriderSoftSweepsAre2017} to generate training and test data for the JPT sample (Japanese individuals from Tokyo) from Phase 3 of the 1000 Genomes dataset \cite{autonGlobalReferenceHuman2015}. 
For the hard and soft sweep classes, the target of selection was located at the center of the simulated chromosome. For the sweep-linked classes the target of selection was located outside of the central “sub-window” (which accounts for 1/11 of the total length of the selected chromosome). For this task we consider a simulated example to be a sweep if the central polymorphism in the sampled window is the target of selection. Sweep-linked classes are defined as having a sweeping allele in the sampled window but not in the center, while the neutral class does not have any selection in the simulation.

For sweep scenarios we condition on the sweeping allele having reached at time $T \sim U(0, 2000)$ generations ago. The sample size was set to 104 haploid individuals. Genotype matrices were either padded or cropped to 5000 polymorphisms. The training, validation, and test sets for the CNN had 236,232, 26,432, and 22,480 replicates respectively.

For the tree sequences, sequences containing more than 128 trees were downsampled as in the other problems, but instead of sequential window sampling we sampled trees by setting the probability that a tree was sampled by the fraction of the chromosome that it spanned, and then sampling with replacement while keeping the original ordering intact. This was done because a large proportion of simulations had $>>$ 128 trees in sequence (the median length across the five categories was 107 with standard deviation of 111.2) and non-linked replicates can have selection almost anywhere along the simulated chromosome, thus contiguous window sampling could often miss the relevant signal. The training, validation, and test sets for the GCN had 224,555, 25,445, and 22,480 replicates respectively, with the latter being the same test set used to assess performance of the CNN. The training, validation and testing data were evenly balanced across the five categories.

\subsubsection{Detecting introgressed loci}

We used the Drosophila simulations generated for \cite{rayIntroUNETIdentifyingIntrogressed2023} as data for a 3-class classifier problem to benchmark each networks' ability to detect the presence and direction of introgression within a genomic region. In brief, we simulated a pulse migration event from \textit{D. simulans} to \textit{D. sechellia} at a random time sampled from $U(0.3, 0.5) \times T$, where $T$ is the population split time inferred by bootstrapped $\partial$a$\partial$i \cite{gutenkunstDiffusionApproximationsDemographic2010} replicate inferences on \textit{D. simulans} inbred and \textit{D. sechellia} wild-caught genomes. In contrast to the parameterization simulated by Ray et al. \cite{rayIntroUNETIdentifyingIntrogressed2023} which used more recent introgression events, we chose the introgression time distribution to be $U(0.3, 0.5) \times T$ to make the problem a more difficult benchmark (because older introgression events have introgressed haplotypes that are less diverged between the donor and recipient population, and also that have had more time to be broken up by recombination events after the introgression event).

For this task genotype matrices are formatted in the shape of (samples, populations, individuals, polymorphisms). Data was padded to 665 polymorphisms or the maximum observed number of polymorphisms in simulated replicates. The median number of trees returned by RELATE (computed from training data) was 68 with a standard deviation of 11.56. For the GCN, the training and validation sets had 232,050 and 25,950 replicates respectively. For the CNN, the training and validation sets had 244,740 and 13,260 replicates respectively. The test set used to compare the methods contained 12,900 replicates, and the training, validation, and test sets were all class-balanced.

\subsubsection{Demographic inference}

As in \cite{flagelUnreasonableEffectivenessConvolutional2019} we tested the ability of our networks to perform demographic inference by simulating a 3-epoch demographic model with 5 parameters: the present-day population size ($N_0$ which was drawn from $U(100, 4 \times 104))$, the time of the most recent population size change $(T_1 \sim U(100, 3500))$, the population size during the middle epoch $(N_1 \sim U(100, 5000))$, the time of the ancient population size change ($T_2$, which was set to $T_1$ plus a value drawn from $U(1, 3500))$, and the ancestral population size $(N_2 \sim U(100, 2 \times 10^4))$. These data were simulated using ms and the corresponding scripts can be found at https://github.com/flag0010/pop\_gen\_cnn/tree/master/demography. For the CNN, the training and validation sets had 94,036 and 5,164 replicates respectively.

The data was padded to 652 polymorphisms or the maximum observed number of polymorphisms in the simulated replicates. Target values were log-scaled and standardized prior to training and testing, with the validation and testing data being standardized according to the mean and standard deviation of the log-scaled training data. For the GCN, the training and validation sets had 82,820 and 9,168 replicates respectively. The median number of trees in sequence was 53 with a standard deviation of 37.0. The test set used to compare the two methods contained 3,995.

\subsection{Data and Code Availability}

All code associated with this work can be found at https://github.com/SchriderLab/TreeSeqPopGenInference. Model weights can be downloaded from \\https://drive.google.com/drive/folders/1WRf8pecfRavOQjmZF7Otr4lXbcouVunm?usp=sharing.

\section{Results}

Our goal is to assess the general effectiveness of the approach of performing population genetic inference from inferred tree sequences. To this end, we evaluate the performance of graph convolutional networks (GCNs), a type of deep neural network that takes graphical data as its input, when applied to inferred tree sequences. We gauge the success of GCNs by comparing their performance to that of CNNs trained on image representations of the input population genetic alignment data—this comparison is an informative measuring stick given that the CNN-image approach has been shown to be highly effective on a number of population genetic problems, as discussed above. We focus our assessment on the four tasks examined by Flagel et al. \cite{flagelUnreasonableEffectivenessConvolutional2019}: estimating historical recombination rates, detecting and classifying selective sweeps, detecting the presence and directionality of introgression, and inferring demographic parameters. We begin with an overview of GCNs and how they differ from CNNs as well as their application to tree sequences before addressing the two approaches' performance on each problem in turn.

\subsection{Graph convolutional networks on tree sequences}

Convolutional neural networks (CNNs) have proved remarkably effective in a variety of fields \cite{erhanScalableObjectDetection2013,hochreiterLongShortTermMemory1997,lecunGradientbasedLearningApplied1998,guRecentAdvancesConvolutional2018,liSurveyConvolutionalNeural2022}, and in recent years have been successfully applied to a number of problems in population genetics \cite{korfmannDeepLearningPopulation2023a,schriderSupervisedMachineLearning2018,sheehanDeepLearningPopulation2016}. These networks work by extracting features from images, or other structured data that can be represented as tensors of two or more dimensions such as audio or video data, using convolutional filters. By using a series of convolution filter layers, often combined with pooling layers that downsample the data, CNNs are able to extract features within the data that are pertinent to the target task. These features are typically then passed as input into a one or more fully connected layers that then emit the final prediction: a vector of values corresponding to either to class membership probabilities in the case of classification (e.g. with each class representing a different possible object present in the image), or to estimated parameter/attribute values in the case of regression. The convolutional filters in a CNN, each of which is smaller than the input to which they are applied, work by striding across the input image and at each step performing a transformation of the portion of the input image that is currently “covered” by the filter \cite{lecunBackpropagationAppliedHandwritten1989}. The transformed value is a function of the pixels in the input image and also the values of the weights of the filter (specifically, the grand sum of the Hadamard product between the filter and the corresponding portion of the input). The filter then moves to another location of the input and repeats the process, until a filtered version of the image has been produced. During training, the weights of the convolutional filters are adjusted to minimize error on the task at hand, thereby producing a set of filters that are capable of extracting low-level structural features (e.g. edges in an image) from the input image that are informative for downstream classification/regression by the rest of the neural network. When multiple layers of convolutions are stacked on top of each other, with the output from one layer passed to the next, features about higher level structures present in the input (e.g. shapes in the image) can be extracted.

GCNs work similarly to CNNs in that they extract features from an input using parameterized filters to convolve over the input space. As the name implies however, rather than an image or “image-like” data structure GCN input consists of a graph. This graph is represented as a pair of matrices: 1) an adjacency matrix describing the structure and connectedness of the nodes in the graph, and 2) a feature matrix containing a vector of attributes for each node in the graph. When centered on a focal node, convolutions in GCNs incorporate information from each of its neighbors, with iterative convolutions passing information from further nodes through information sharing and, in the case of graph attention networks (GATs), attention. GCNs are thus a generalization of CNNs, with the latter being a special case where each pixel in the input corresponds to a node within a lattice graph structure. The design of GCNs allows them to learn from explicit connectedness in a graph to make inferences based on topology and features. Although a relatively new form of neural networks, GCNs have been shown to have excellent accuracy on tasks where graph data is relevant and have become a widely-researched type of neural network \cite{zhouGraphNeuralNetworks2020}, however they have had only limited use in population genetics applications \cite{korfmannDeepLearningPopulation2023a,korfmannSimultaneousInferenceDemography2023}.

Because genealogies are a type of graph, the trees in a tree sequence are a fitting input for a GCN. However, in order to extract the most information possible for inference the spatial relationship between trees must be considered as well. To accomplish this we built a hybrid GCN-RNN architecture to learn both the graph structure of each tree in a sequence as well as the spatial relationship of trees along a chromosome. Each tree is passed through six graph convolution layers which extract information from an individual tree in the sequence comprising of node features (node age, number of mutations, population label; see Methods for more details) and the connectedness of the graph through adjacency matrix (represented by edge indices). After graph convolution, we pass the node features through two RNNs (recurrent neural networks), the first of which compresses the features from each tree into a single feature vector and the second compresses the features from each sequence of trees into a single vector. This vector is then used as input to a fully-connected network (MLP) that is used to optimize output for a given task. This combination of models optimizing for a single task results in an architecture able to learn both the spatial relatedness of nodes within a tree and their relevance as well as the larger-scale structured information of the tree sequence as a whole. This workflow is displayed in Figure 2.1.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth{figures/ch2/Workflow.pdf}
    \caption[Overview of the GCN architecture.]{Overview of the GCN architecture.A) Each tree in the input tree sequence contains n nodes each summarized by f features, and is thus represented by an n × f node feature matrix per-tree, which is affine-transformed in the “node embedding” layer of the neural network (Table 2.1). B) For each tree, the affine-transformed node feature matrix is concatenated to a vector of edge indices specifying the tree topology, and also to the original feature matrix (i.e. a “skip connection”). This concatenated matrix is then input to multiple graph convolutional layers with skip connections. C) For each tree, the features from graph convolution layers are then fed as input to a GRU, resulting in a compressed, learned vector representation of the tree and its features. D) These compressed vector representations are concatenated to a “tree summary vector” that encodes summaries of the size and shape of the tree, as well as the size and location of the chromosomal segment corresponding to the tree. The resulting vectors for all trees are then concatenated together into one matrix corresponding to the entire tree sequence, and passed through another GRU layer that produces a flattened vector representation of the entire tree sequence. E) This vector is then concatenated to a “tree-sequence summary vector” containing information about the distribution of values in the original tree summary vectors and the number of trees in the original sequence (which may be less than the padded length of this vector). This vector is then passed through multiple fully-connected layers culminating in a final output layer tailored to the target task. Throughout the diagram, solid lines with arrows show the flow of information through layers that transform the input, while dashed lines with arrows show the flow of information that has been unaltered by any preceding layers in the network.}
    \label{fig:enter-label}
\end{figure}

\input{tables/ch2/t1}

\subsection{Test Cases}
\subsubsection{GCNs can leverage tree-sequence data to accurately estimate recombination rates}

Estimating historical recombination rates is a deeply important task to population genetics as the recombination rate modulates key evolutionary processes and also affects our ability to make inferences about these processes. For example, when considering the effects of selection, the recombination rate between loci influences the degree of selective interference between them \cite{hillEffectLinkageLimits1966}. Moreover, in the case of positive selection, the signatures of selection are much more difficult to detect in the presence of high rates of recombination due to the rapid exchange of alleles, which dampens the hitchhiking effect by preserving more genetic diversity in the vicinity of the selected site \cite{shrinerPotentialImpactRecombination2003}. Because the recombination rate has a direct influence on the correlation between alleles in a sample of genomes, population genomic data can in principle be used to get an accurate estimate of the recombination rate landscape across the genome \cite{adrionPredictingLandscapeRecombination2020,autonRecombinationRateEstimation2007,chanGenomeWideFineScaleRecombination2012,gaoNewSoftwareFast2016,hudsonTwolocusSamplingDistributions2001}. 

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{figures/ch2/Slide1}}
    \caption[Predicted versus true recombination rates for the (A) CNN and (B) GCN architectures]{Predicted versus true recombination rates for the (A) CNN and (B) GCN architectures. Predictions are plotted against a reference diagonal dashed line, and each panel shows the root mean squared error (RMSE) and coefficient of determination ($R^2$) for the given method.}
    \label{fig:enter-label}
\end{figure}

To assess the utility of tree-sequence based recombination rate inference, we trained our GCN for this task and compared its performance to the CNN previously shown by \cite{flagelUnreasonableEffectivenessConvolutional2019} to accurately estimate recombination rates. Specifically, we simulated data using the same parameter values as in \cite{flagelUnreasonableEffectivenessConvolutional2019}) to generate samples of phased chromosomes which we used to estimate the crossover rate per base pair per meiosis by either passing these data directly into a CNN or through a tree-sequence inference procedure before passing the resulting trees into the GCN. In the context of tree sequences, recombination plays a very direct role in the structure of the data: most recombination events will alter the tree topology, and thus the number of trees in the tree sequence minus 1 is equal to the number of historical recombination events that could possibly be observed. However, because inferred tree sequences will contain errors, it is not clear whether we should expect methods that examine information from inferred tree sequences to estimate recombination to be more accurate than methods that directly examine the haplotype data. As shown in Figure 2.2, our GCN using tree sequences performed slightly better than our CNN (RMSE = 5.54E-03 compared to RMSE = 6.01E-03) that takes the population genetic alignment as input. Thus, the presence of error in inferred tree sequences does not preclude the accurate inference of recombination rates from such data.

\subsubsection{GCNs dramatically outperform CNNs in detecting selective sweeps}

The ability to detect loci affected by positive selection is a longstanding problem in population genetics \cite{stephanSelectiveSweeps2019}. Selective sweeps occur when positive selection affecting an allele causes it to rapidly increase in frequency in a population, creating a valley of diversity that recovers at increasing genetic distances from the selected site \cite{kaplanHitchhikingEffectRevisited1989,smithHitchhikingEffectFavourable1974}. Sweeps also produce an excess of high-frequency derived alleles \cite{fayHitchhikingPositiveDarwinian2000}, a deficit of haplotypic diversity \cite{fayHitchhikingPositiveDarwinian2000,hudsonEvidencePositiveSelection1994,sabetiDetectingRecentPositive2002}, and elevated linkage disequilibrium \cite{kellyTestNeutralityBased1997,kimLinkageDisequilibriumSignature2004} near the target of selection. Recently, there has been great interest in characterizing and detecting the signatures of “soft sweeps” \cite{hermissonSoftSweepsUnderstanding2017} which may be common in large, rapidly evolving populations \cite{garudRecentSelectiveSweeps2015,hermissonSoftSweeps2005,karasovEvidenceThatAdaptation2010}, and which produce somewhat different signatures than classic “hard” sweeps \cite{bergCoalescentModelSweep2015,przeworskiSignaturePositiveSelection2005}. Recently, a number of machine learning methods have been designed to detect the multidimensional signatures of both of these types of sweeps \cite{caldasInferenceSelectiveSweep2022,hejaseDeepLearningApproachInference2022,lauterburVersatileDetectionDiverse2022,mughalLocalizingClassifyingAdaptive2019,pybusHierarchicalBoostingMachinelearning2015,schriderHICRobustIdentification2016,whitehouseTimesweeperAccuratelyIdentifying2022,whitehouseTimesweeperAccuratelyIdentifying2023}. 

However, it should be noted that the presence of all of these signatures of selection is a side effect of the profound effect that positive selection has on genealogies in the genomic vicinity of the selected locus. The shapes and sizes of trees are affected by selection in that the genealogies of loci near the selected site have a very short branches, especially internal branches, while genealogies at intermediate distances from the sweeping allele may have a subset of long branches as a consequence of recombination during the sweep \cite{fayHitchhikingPositiveDarwinian2000,przeworskiSignaturePositiveSelection2005,stephanSelectiveSweeps2019}. Moreover, because of the reduced time period for historical recombination events to occur during the sweep, trees near the selected locus will correspond to a larger stretch of sequence than those from more distant loci that have experienced more recombination \cite{degiorgioSpatiallyAwareLikelihood2022,ferrer-admetllaDetectingIncompleteSoft2014,przeworskiSignaturePositiveSelection2005,sabetiDetectingRecentPositive2002}. Thus, tree sequences may provide a great deal of information about recent positive selection. 

To assess the potential of GCNs for detecting sweeps, we used the same simulated benchmarking scenario as \cite{flagelUnreasonableEffectivenessConvolutional2019}, and trained our networks to classify regions as undergoing a recent hard selective sweep (from a de novo mutation), being linked to a recent hard sweep, undergoing a soft sweep (from standing variation), being linked to a soft sweep, or evolving neutrally. Specifically, we simulated data under the JPT model from \cite{schriderSoftSweepsAre2017} to generate training, testing, and validation sets as described in the Methods. We find that the GCN architecture outperforms the CNN for all classes except neutrality, where their performance is equal (Figure 2.3). Indeed, the GCN obtains an average increase of 6.6\% of test samples assigned to the proper class (the confusion matrix diagonal) relative to the CNN (65.5\% accuracy for the GCN vs 58.8\% for the CNN). Perhaps more strikingly, the GCN exhibits a sizable increase relative to the CNN in the area under the receiver operating characteristic (ROC) curve (AUROC) and in average precision (AP, equal to the area under the precision-recall curve) for distinguishing sweeps from neutrality (difference in AUROC of 0.085 and difference in AP of 0.151; Figure 2.4A, B). The increase in accuracy is less notable for distinguishing between hard and soft sweeps, with an AUROC increase of 0.004 and AP increase of 0.007 for the GCN compared to the CNN (Figure 2.4C, D). 

\begin{figure}
    \centering
    \begin{center}
    \makebox[\textwidth][c]{\includegraphics[width=1.6\textwidth]{figures/ch2/Slide2}}
    \caption[Receiver operating characteristic (ROC) and precision-recall (PR) curves summarizing performance of the CNN and GCN on the sweep-detection task]{Receiver operating characteristic (ROC) and precision-recall (PR) curves summarizing performance of  the CNN (blue) and GCN (orange) on the sweep-detection task. (A) ROC curves showing how well each method distinguishes between selective sweeps (whether hard or soft) and unselected regions (whether sweep-linked or neutral). The curves show the true and false positive rates for the CNN (blue) and GCN (orange) at varying classification thresholds. The score used to classify a window as a sweep vs. unselected at a given threshold was the sum of the neural network’s class membership probabilities for the “hard sweep” and “soft sweep” classes to yield a combined “sweep” probability. The area under the ROC curve (AUC) for each network is shown in the inset. B) Precision-recall curves showing the two network’s performance on the same “sweep vs. unselected” task, here showing the precision (i.e. the positive predictive value, or fraction of regions classified as sweeps that truly were sweeps) and recall (the true positive rate) at varying classification thresholds. The average precision (AP, which is equal to the area under the PR curve) is shown in the inset. C) ROC curves for distinguishing between hard and soft sweeps, with hard sweeps treated as positives and soft sweeps as negatives for this binary classification task. D) Precision-recall curves for the “hard vs. soft” task.}
    \end{center}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{figures/ch2/Slide3}}
    \caption[Confusion matrices showing performance of the CNN architecture and GCN architecture on the sweep detection task]{Confusion matrices showing performance of the CNN architecture and GCN architecture on the sweep detection task. A) Confusion matrix summarizing the CNN’s performance on a held-out test set. For this problem, there are five classes specifying the type of sweep (hard versus soft), whether a focal window is impacted by direct selection within the window or linked selection outside of the window (sweep vs. linked), or neutrally evolving. Each entry in the matrix shows the fraction of simulated examples under a given class (the True Class, specified by the row of the matrix) that were assigned to a given class by the network (the Predicted Class, specified by the column of the matrix). Diagonal entries thus represent correct classifications, and errors are represented in the off-diagonal entries, and each row sums to 1. B) Confusion matrix summarizing the GCN’s performance on the same test set.}
    \label{fig:enter-label}
    
    \label{fig:enter-label}
\end{figure}



\subsubsection{GCNs perform similarly to CNNs on the task of detecting introgression}

A variety of approaches have therefore been developed to identify the signatures of introgression between pairs of closely related species \cite{hudsonEstimationLevelsGene1992,husonReconstructionReticulateNetworks2005,rosenzweigPowerfulMethodsDetecting2016}, including several machine learning methods \cite{gowerDetectingAdaptiveIntrogression2021,rayIntroUNETIdentifyingIntrogressed2023,schriderSupervisedMachineLearning2018a,schriderSupervisedMachineLearning2018a}.

Previously, Flagel et al. (2019) showed that a CNN substantially outperforms FILET, a supervised machine learning method that detects introgression between closely related species with impressive accuracy (Schrider et al. 2018). Here we compared the performance of the tree-sequence based GCN to our CNN on simulated data modeling gene flow between Drosophila simulans and Drosophila sechellia (see Methods for details). We find that both networks perform well (Figures 5,6), however as with the other tasks the GCN outperforms the CNN. The overall error profile is very similar for the two architectures (Figure 2.5) indicating that the GCN's improvement over the CNN (86.2\% accuracy for the GCN versus 85.1\% for the CNN) is not due to any specific class, but rather due to slight accuracy increases (1–2\%) across all three classes. The GCN has an AUROC increase of 0.004 and AP increase of 0.002 when measuring accuracy distinguishing between genomic windows with and without introgression, and an increase in AUROC of 0.004 and AP of 0.007 when inferring the direction of introgression.


\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{figures/ch2/Slide5}}
    \caption[Confusion matrices showing the neural network’s accuracies on the introgression inference task]{Confusion matrices showing the neural network’s accuracies on the introgression inference task. Confusion matrices are shown in the same manner as for Figure 2.3, but here the three classes are introgression from Drosophila simulans to D. sechellia, introgression from D. sechellia to D. simulans, and no introgression. A) Confusion matrix for the CNN. B) Confusion matrix for the GCN.}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.6\textwidth]{figures/ch2/Slide4}}
    \caption[Receiver operating characteristic (ROC) and precision-recall (PR) curves for the introgression detection task]{Receiver operating characteristic (ROC) and precision-recall (PR) curves for the introgression detection task. A) ROC curves summarizing the performance of the CNN (blue) and GCN (orange) on the task of distinguishing between windows with introgression (in either direction) and without introgression. The area under the ROC curve (AUC) for each network is shown in the inset. B) Precision-recall curves for the task of distinguishing between windows with and without introgression. The average precision (AP, which is equal to the area under the PR curve) is shown in the inset. C) ROC curves for the task of distinguishing between introgression from D. simulans to D. sechellia and introgression from D. sechellia to D. simulans. D) Precision-recall curves for the task of inferring the direction of introgression.}
    \label{fig:enter-label}
\end{figure}

\subsubsection{GCNs show improvement compared to CNNs on a demographic inference task}

Perhaps one of the most widely studied tasks in population genetics is the inference of demographic models and their associated parameters from. Many statistical approaches have been developed to allow researchers to estimate a population's history of size changes, splitting and merging events, and expansions and contractions \cite{browningAccurateNonparametricEstimation2015,excoffierFastsimcoal2DemographicInference2021,gutenkunstInferringJointDemographic2009,gutenkunstInferringJointDemographic2009,gutenkunstDiffusionApproximationsDemographic2010,kammEfficientlyInferringDemographic2020,liInferenceHumanPopulation2011,santiagoRecentDemographicHistory2020,terhorstRobustScalableInference2017} These include several deep learning methods that have been designed in recent years \cite{sanchezDeepLearningPopulation2021,wangAutomaticInferenceDemographic2021}, one of which applies graph convolutions to the tree sequence for inference \cite{korfmannSimultaneousInferenceDemography2023}. To compare the performance of our GCN to the CNN on a demographic inference task, we reexamined the 3-epoch model presented in \cite{flagelUnreasonableEffectivenessConvolutional2019}. We simulated data under this model and assessed each network's accuracy in estimating its 5 parameters: the population sizes in each of the 3 epochs, and the 2 transition times between these epochs. 

We found that the GCN architecture outperforms the CNN on all estimations except the ancestral population size ($N_2$) where the two methods perform equally as measured by both root mean squared error (RMSE) and the coefficient of determination (R2) on test data (Figure 7). With respect to population size inference, the difference in accuracy between the GCN and CNN was most pronounced for the present-day population size ($N_0$; improvement in RMSE and R2 of 0.046 and 0.048, respectively, for the GCN over the CNN), with the middle epoch's population size inference ($N_1$) showing smaller differences in accuracy (difference in RMSE and R2 of 0.026 and 0.045, respectively), and again no difference between the two methods for the ancestral size. For the inferred times of population size changes, we again observed greater difference in accuracy for the inferred time of the most recent event ($T_1$; difference in RMSE and R2 of 0.067 and 0.069, respectively) than for the inferred time of the more ancient size change ($T_2$; difference in RMSE and R2 of 0.043 and 0.047). This improvement in accuracy is somewhat modest (average RMSE decrease of 0.055 and average R2 decrease of 0.058 across all parameters), and we remind the reader that this test case involves the inference of demographic parameters from a single genomic region as a proof of concept rather than examining genome-wide data as is typically done in practice. Nonetheless, these results demonstrate the GCN's ability to examine inferred tree sequences and extract information relevant to past demographic events at least as well as a CNN directly examining the population genetic alignment. 

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=2.0\textwidth]{figures/ch2/Slide6}}
    \caption[Performance of the CNN and GCN on the task of demographic parameter estimation on a held-out test set]{Performance of the CNN and GCN on the task of demographic parameter estimation on a held-out test set. The top row shows the predicted versus true values for the CNN (panel A) and GCN (panel B) for the present-day population size N0, and subsequent rows show the same for the time of the most recent population size change ($T1$, panels C and D), the second population size ($N1$, panels E and F), the time of the more ancient population size change ($T2$, panels G and H), and the ancestral population size ($N2$, panels C and D). Each panel also shows the root mean squared error (RMSE) and coefficient of determination ($R^2$) for the given method on the given parameter. Values are log-scaled for visual clarity and plotted against a reference diagonal dashed line.}
    \label{fig:enter-label}
\end{figure}

\subsection{Performance and computational efficiency}
Perhaps the most enticing attribute of tree sequences is that they can provide extremely compact representations of population genomic data \cite{kelleherInferringWholegenomeHistories2019}, leading to the possibility that they can be leveraged to reduce the computational burden of evolutionary inference pipelines. We therefore compared the computational cost of our GCN and CNN on the problems examined above. With respect to preprocessing, our GCN's pipeline has an advantage over our CNN pipeline in that tree sequences are relatively fast to infer compared to the sorting and matching steps that we perform on alignments before passing them into the CNN. For instance, computing Relate outputs for 1000 simulated replicates of length 10 kb from the introgression case took on average $\sim$ 5 minutes for a single CPU core vs. $\sim$ 21 minutes to seriate and match the same number replicates. In terms of memory efficiency, the size of a single replicate of from the introgression data set after the tree-sequence inference step was 3.25 times the size of the original simulations (Tables 2.2, 2.3). Similar trends are observed for the recombination and demography problems. For sweep detection, the GCN requires slightly less memory than the CNN, but this is a consequence of our downsampling of the tree sequence for the GCN (Methods), while the CNN examines the entire input alignment. 

Training time is also an important consideration when choosing deep learning architecture. We note that our CNN, due to the smaller input sizes and the greater efficiency of image convolutions over graph convolutions with current hardware and software, are faster to train and usually require less RAM (Tables 2, 3). This is in spite of the fact that our choice of CNN has almost 2 orders of magnitude more learnable parameters than our proposed GCN architecture (21,280,965 vs. 228,837 respectively). However, we find both approaches to be accomplishable on consumer-grade hardware, i.e. on a single GPU with $\sim$ 12 Gb of RAM. 

\input{tables/ch2/t2}

\input{tables/ch2/t3}


\section{Discussion}

The size and quantity of population genetic datasets are increasingly large, making downstream analysis more computationally intensive. As researchers develop new, more compact data structures for population genetics, new methods must be developed to fully take advantage of these structures' potential to tackle long standing problems in the field with greater computational efficiency and perhaps better accuracy as well. Tree sequences have become increasingly popular, both in the context of estimation from empirical data and in simulation \cite{hallerTreesequenceRecordingSLiM2019,kelleherEfficientPedigreeRecording2018,ralphEfficientlySummarizingRelationships2020}. However, there has yet to be a fundamental shift in population genetics inference approaches away from less efficient data structures \cite{korfmannDeepLearningPopulation2023a}. In principle, tree sequences have a key advantage over genotype information in that they can represent the complete evolutionary history of a sample of genome sequences, making them an attractive avenue for downstream evolutionary inference. 

Several previous studies have presented deep learning approaches for analyzing tree-sequence data, with initial methods using predefined sets of features summarizing the properties of trees in the sequence that are then fed into a neural network \cite{hejaseDeepLearningApproachInference2022,moDomainadaptiveNeuralNetworks2023a,pearsonLocalAncestryInference2023}. While these approaches showed great promise, because they examine summaries of the trees rather than the tree itself, they could potentially omit valuable information present in the tree sequence. Just as deep learning-based methods operating directly on genotype/haplotype matrices are able to extract more useful information than vectors of statistics designed to summarize aspects of a sample of genotypes/haplotypes (e.g. allele frequencies, linkage disequilibrium) \cite{flagelUnreasonableEffectivenessConvolutional2019,kernDiploSHICUpdated2018}, graph neural networks operating directly on tree sequences may perform better than methods using predefined tree summaries (e.g. the number of lineages present in the tree within each of a set of non-overlapping time intervals \cite{hejaseDeepLearningApproachInference2022}). Indeed, this approach was used successfully by \cite{korfmannSimultaneousInferenceDemography2023}. for demographic inference. Here, we adopt a similar strategy, using a GCN to show that tree sequence-based inference appears to be at least as powerful of a framework for evolutionary inference as deep neural networks acting directly on a population genetic alignment.

The main caveat with using tree sequences is that in empirical data the true evolutionary history of a sample can never be known, and therefore tree sequences must be estimated from genotypes \cite{kelleherInferringWholegenomeHistories2019,mahmoudiBayesianInferenceAncestral2022}. As it stands, inference methods can produce highly inaccurate tree sequences when benchmarked on simulated data \cite{zhangBiobankscaleInferenceAncestral2023a}, suggesting that using tree sequences as input for further inferences may be problematic. Moreover, the paradigm of inferring a population sample's tree sequence to use as input for a downstream inference task does on the face of it appear to be unnecessarily complicated, and appears to violate the following imperative from Vapnik: “When solving a problem of interest, do not solve a more general problem as an intermediate step” \cite{vapnikEstimationDependencesBased2006}. For example, if we wish to infer whether a given locus has experienced a selective sweep, it may not be desirable to first infer the full evolutionary history of every segment of DNA along a large stretch of chromosome surrounding this locus, as this would clearly be a more challenging task than directly searching for evidence of recent positive selection.

Why then, might tree sequence-based inference still be generally highly effective, as our results seem to suggest? First, it seems probable that incorrect tree sequences contain information that, although incorrectly describing the true evolutionary history of a sample, can be useful for population genetic inference. In addition, tree sequences allow for a more concise representation of population genetic data than a genotype matrix. Moreover, tree sequence inference may serve as a form of de-noising by aggregating data across sites in a region in order to extract a meaningful pattern that may not be easily observable directly from sequence data (especially in the presence of sequencing error), even if that pattern does not describe the true evolutionary history. In any case, supervised machine learning methods combined with tree sequences can be especially powerful because one can train the machine learning model on tree sequences that are inferred from simulated data (as we have done here), rather than the true tree sequences, so that the training tree sequences may exhibit a similar error profile as tree sequences inferred from real data. In this framework, it does not matter whether the tree sequences are correct, but only that they contain information useful for the task at hand. Moreover, the impact of any remaining fundamental differences between tree sequences from simulated vs. real data could potentially be mitigated via the machine learning technique of domain adaptation \cite{ganinUnsupervisedDomainAdaptation2015}, wherein a network can be trained to learn to ignore systematic differences between simulated and real data before downstream prediction \cite{moDomainadaptiveNeuralNetworks2023a}. Thus, in spite of the somewhat unusual workflow, relying on inferred tree sequences as input to supervised machine learning methods may in fact prove to be an especially powerful and robust strategy.

We note that although we have provided an attempt at solving multiple population genetics inference tasks the number supervised machine learning approaches to population genetics has ballooned in recent years such as detecting selective sweeps \cite{hejaseDeepLearningApproachInference2022,kernDiploSHICUpdated2018,whitehouseTimesweeperAccuratelyIdentifying2023}, detecting introgression \cite{gowerDetectingAdaptiveIntrogression2021,rayIntroUNETIdentifyingIntrogressed2023,schriderSupervisedMachineLearning2018a}, predicting locations of recombination \cite{adrionPredictingLandscapeRecombination2020} among others (reviewed in \cite{flagelUnreasonableEffectivenessConvolutional2019,korfmannDeepLearningPopulation2023a,schriderSupervisedMachineLearning2018}. The encouraging benchmarking results from our GCN suggest that machine learning methods using tree sequences will likely prove to be a powerful addition to this ever-growing suite of tools and tasks. 

One important limitation of our work is that, although our networks were able to be trained and executed on consumer-grade hardware, we have not made exhaustive attempts to improve computational efficiency. Indeed, we find that our CNN is more efficient than the GCN in terms of both memory usage and training time, with the current advantage of the GCN being its faster preprocessing time. However, the latter is a consequence of our choice to sort our alignments in order to improve performance, a computationally expensive process \cite{rayIntroUNETIdentifyingIntrogressed2023} and it is important to note that the cost of this step will depend on the sorting algorithm used and the size and number of alignments to be sorted. Moreover, at least some problems, exchangeable neural networks that do not require any sorting may be used \cite{chanLikelihoodFreeInferenceFramework2018}. However, we note that the advantage in compactness afforded by tree sequences is not observed at small sample sizes \cite{kelleherInferringWholegenomeHistories2019}, and the tasks examined here all used small to modest sample sizes. Thus, our conclusions about the relative efficiency of GCNs vs. CNNs may not hold for tasks requiring much larger samples. Further investigations into optimizations of both architecture and processing algorithms may increase the favorability of the GCN in all aspects. future efforts experimenting with more compact tree representations \cite{hallerTreesequenceRecordingSLiM2019,kelleherEfficientPedigreeRecording2018,kelleherInferringWholegenomeHistories2019,mahmoudiBayesianInferenceAncestral2022,ralphEfficientlySummarizingRelationships2020} and different neural network architectures may yield improvements in this area.

Finally, we note that we have only explored a relatively small number of population genetic inference tasks that are of particularly broad interest to researchers. We therefore encourage exploration of tree sequence-based deep learning inference as a general framework for population genetic inference. Our hope is that this may not only lead to improved accuracy on the problems examined here, but that tree sequence-deep learning paradigm may prove useful for a wider array of problems and, as has proved to be the case for alignment-based deep learning applications, even empower researchers to tackle new problems \cite{batteyVisualizingPopulationStructure2021,bookerThisPopulationDoes2023,korfmannDeepLearningPopulation2023a,smithDispersalInferencePopulation2023,whitehouseTimesweeperAccuratelyIdentifying2023,yelmenCreatingArtificialHuman2021}
